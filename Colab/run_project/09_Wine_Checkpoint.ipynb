{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"09_Wine_Checkpoint.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMlYWg0esjPZ21v8gkBQQgY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"uQTbIWvOyXwb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594286138181,"user_tz":-540,"elapsed":2235,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}},"outputId":"35327ebb-28a4-4ca2-dd0e-b8776a4c9148"},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import ModelCheckpoint"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"YivfL9rh0CqF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594286138185,"user_tz":-540,"elapsed":2191,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"ue7QQC1D0Myj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594286138189,"user_tz":-540,"elapsed":2150,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["# seed 값 설정\n","seed = 3\n","\n","np.random.seed(seed)\n","tf.random.set_seed(seed)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMP-7q-A0R5j","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1594286153857,"user_tz":-540,"elapsed":17780,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}},"outputId":"5bc8ca79-94a6-4453-9946-ec67bba99d95"},"source":["from google.colab import files\n","\n","Uploaded = files.upload()\n","\n","df_pre = pd.read_csv(\"wine.csv\", header=None)\n","df = df_pre.sample(frac = 1)"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-61a5c703-9ad4-4fa8-808f-b6a4f8215d1a\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-61a5c703-9ad4-4fa8-808f-b6a4f8215d1a\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving wine.csv to wine (1).csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sBwBXDNR0nGg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594286153860,"user_tz":-540,"elapsed":17765,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["dataset = df.values\n","X = dataset[:, 0:12]\n","Y = dataset[:, 12]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDFMcOMa0uh7","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594286154950,"user_tz":-540,"elapsed":18841,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["# 모델 설정\n","model = Sequential()\n","model.add(Dense(30, input_dim = 12, activation=\"relu\"))\n","model.add(Dense(12, activation=\"relu\"))\n","model.add(Dense(8, activation=\"relu\"))\n","model.add(Dense(1, activation=\"sigmoid\"))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tcy9OmFR1K-X","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594286154953,"user_tz":-540,"elapsed":18828,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["# 모델 컴파일\n","model.compile(loss = \"binary_crossentropy\",\n","              optimizer = \"adam\",\n","              metrics = [\"accuracy\"])"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wGxZEA31Up0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594286154957,"user_tz":-540,"elapsed":18819,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["# 모델 저장 폴더 설정\n","MODEL_DIR = \"./model/\" # 모델을 저장하는 폴더\n","\n","if not os.path.exists(MODEL_DIR): # 만일 위의 폴더가 존재하지 않으면\n","  os.mkdir(MODEL_DIR) # 이 이름의 폴더를 만들어줌"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FtOlM2nyZ3l","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594286154960,"user_tz":-540,"elapsed":18807,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["# 모델 저장 조건 설정\n","modelpath = \"./model/{epoch:02d}-{val_loss:.4f}.hdf5\"\n","checkpointer = ModelCheckpoint(filepath = modelpath,\n","                               monitor = \"val_loss\", verbose = 1, save_best_only = True)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpFRSLTFzEtj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594286175381,"user_tz":-540,"elapsed":39210,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}},"outputId":"e9b1fa36-5d2d-4e11-a2c2-d15a2e9244a7"},"source":["model.fit(X, Y, validation_split=0.2, epochs = 200, batch_size=200, verbose = 0, callbacks=[checkpointer])"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\n","Epoch 00001: val_loss improved from inf to 0.35566, saving model to ./model/01-0.3557.hdf5\n","\n","Epoch 00002: val_loss improved from 0.35566 to 0.31362, saving model to ./model/02-0.3136.hdf5\n","\n","Epoch 00003: val_loss improved from 0.31362 to 0.28975, saving model to ./model/03-0.2898.hdf5\n","\n","Epoch 00004: val_loss improved from 0.28975 to 0.27080, saving model to ./model/04-0.2708.hdf5\n","\n","Epoch 00005: val_loss improved from 0.27080 to 0.24785, saving model to ./model/05-0.2478.hdf5\n","\n","Epoch 00006: val_loss improved from 0.24785 to 0.22917, saving model to ./model/06-0.2292.hdf5\n","\n","Epoch 00007: val_loss improved from 0.22917 to 0.21587, saving model to ./model/07-0.2159.hdf5\n","\n","Epoch 00008: val_loss improved from 0.21587 to 0.20433, saving model to ./model/08-0.2043.hdf5\n","\n","Epoch 00009: val_loss improved from 0.20433 to 0.19554, saving model to ./model/09-0.1955.hdf5\n","\n","Epoch 00010: val_loss improved from 0.19554 to 0.18873, saving model to ./model/10-0.1887.hdf5\n","\n","Epoch 00011: val_loss improved from 0.18873 to 0.18461, saving model to ./model/11-0.1846.hdf5\n","\n","Epoch 00012: val_loss did not improve from 0.18461\n","\n","Epoch 00013: val_loss improved from 0.18461 to 0.17752, saving model to ./model/13-0.1775.hdf5\n","\n","Epoch 00014: val_loss improved from 0.17752 to 0.17285, saving model to ./model/14-0.1728.hdf5\n","\n","Epoch 00015: val_loss improved from 0.17285 to 0.16561, saving model to ./model/15-0.1656.hdf5\n","\n","Epoch 00016: val_loss improved from 0.16561 to 0.16201, saving model to ./model/16-0.1620.hdf5\n","\n","Epoch 00017: val_loss improved from 0.16201 to 0.15792, saving model to ./model/17-0.1579.hdf5\n","\n","Epoch 00018: val_loss improved from 0.15792 to 0.15299, saving model to ./model/18-0.1530.hdf5\n","\n","Epoch 00019: val_loss improved from 0.15299 to 0.14908, saving model to ./model/19-0.1491.hdf5\n","\n","Epoch 00020: val_loss improved from 0.14908 to 0.14601, saving model to ./model/20-0.1460.hdf5\n","\n","Epoch 00021: val_loss improved from 0.14601 to 0.14346, saving model to ./model/21-0.1435.hdf5\n","\n","Epoch 00022: val_loss improved from 0.14346 to 0.13738, saving model to ./model/22-0.1374.hdf5\n","\n","Epoch 00023: val_loss improved from 0.13738 to 0.13364, saving model to ./model/23-0.1336.hdf5\n","\n","Epoch 00024: val_loss improved from 0.13364 to 0.12967, saving model to ./model/24-0.1297.hdf5\n","\n","Epoch 00025: val_loss did not improve from 0.12967\n","\n","Epoch 00026: val_loss improved from 0.12967 to 0.12514, saving model to ./model/26-0.1251.hdf5\n","\n","Epoch 00027: val_loss improved from 0.12514 to 0.12287, saving model to ./model/27-0.1229.hdf5\n","\n","Epoch 00028: val_loss improved from 0.12287 to 0.11949, saving model to ./model/28-0.1195.hdf5\n","\n","Epoch 00029: val_loss improved from 0.11949 to 0.11618, saving model to ./model/29-0.1162.hdf5\n","\n","Epoch 00030: val_loss did not improve from 0.11618\n","\n","Epoch 00031: val_loss improved from 0.11618 to 0.10904, saving model to ./model/31-0.1090.hdf5\n","\n","Epoch 00032: val_loss did not improve from 0.10904\n","\n","Epoch 00033: val_loss improved from 0.10904 to 0.10267, saving model to ./model/33-0.1027.hdf5\n","\n","Epoch 00034: val_loss improved from 0.10267 to 0.10007, saving model to ./model/34-0.1001.hdf5\n","\n","Epoch 00035: val_loss improved from 0.10007 to 0.09821, saving model to ./model/35-0.0982.hdf5\n","\n","Epoch 00036: val_loss improved from 0.09821 to 0.09791, saving model to ./model/36-0.0979.hdf5\n","\n","Epoch 00037: val_loss improved from 0.09791 to 0.09553, saving model to ./model/37-0.0955.hdf5\n","\n","Epoch 00038: val_loss improved from 0.09553 to 0.09297, saving model to ./model/38-0.0930.hdf5\n","\n","Epoch 00039: val_loss did not improve from 0.09297\n","\n","Epoch 00040: val_loss did not improve from 0.09297\n","\n","Epoch 00041: val_loss improved from 0.09297 to 0.08878, saving model to ./model/41-0.0888.hdf5\n","\n","Epoch 00042: val_loss did not improve from 0.08878\n","\n","Epoch 00043: val_loss did not improve from 0.08878\n","\n","Epoch 00044: val_loss did not improve from 0.08878\n","\n","Epoch 00045: val_loss did not improve from 0.08878\n","\n","Epoch 00046: val_loss improved from 0.08878 to 0.08668, saving model to ./model/46-0.0867.hdf5\n","\n","Epoch 00047: val_loss improved from 0.08668 to 0.08288, saving model to ./model/47-0.0829.hdf5\n","\n","Epoch 00048: val_loss improved from 0.08288 to 0.08230, saving model to ./model/48-0.0823.hdf5\n","\n","Epoch 00049: val_loss did not improve from 0.08230\n","\n","Epoch 00050: val_loss did not improve from 0.08230\n","\n","Epoch 00051: val_loss did not improve from 0.08230\n","\n","Epoch 00052: val_loss did not improve from 0.08230\n","\n","Epoch 00053: val_loss did not improve from 0.08230\n","\n","Epoch 00054: val_loss did not improve from 0.08230\n","\n","Epoch 00055: val_loss did not improve from 0.08230\n","\n","Epoch 00056: val_loss improved from 0.08230 to 0.07756, saving model to ./model/56-0.0776.hdf5\n","\n","Epoch 00057: val_loss improved from 0.07756 to 0.07539, saving model to ./model/57-0.0754.hdf5\n","\n","Epoch 00058: val_loss improved from 0.07539 to 0.07413, saving model to ./model/58-0.0741.hdf5\n","\n","Epoch 00059: val_loss did not improve from 0.07413\n","\n","Epoch 00060: val_loss did not improve from 0.07413\n","\n","Epoch 00061: val_loss improved from 0.07413 to 0.07238, saving model to ./model/61-0.0724.hdf5\n","\n","Epoch 00062: val_loss did not improve from 0.07238\n","\n","Epoch 00063: val_loss did not improve from 0.07238\n","\n","Epoch 00064: val_loss improved from 0.07238 to 0.07113, saving model to ./model/64-0.0711.hdf5\n","\n","Epoch 00065: val_loss improved from 0.07113 to 0.07022, saving model to ./model/65-0.0702.hdf5\n","\n","Epoch 00066: val_loss improved from 0.07022 to 0.06964, saving model to ./model/66-0.0696.hdf5\n","\n","Epoch 00067: val_loss did not improve from 0.06964\n","\n","Epoch 00068: val_loss improved from 0.06964 to 0.06861, saving model to ./model/68-0.0686.hdf5\n","\n","Epoch 00069: val_loss did not improve from 0.06861\n","\n","Epoch 00070: val_loss improved from 0.06861 to 0.06829, saving model to ./model/70-0.0683.hdf5\n","\n","Epoch 00071: val_loss did not improve from 0.06829\n","\n","Epoch 00072: val_loss did not improve from 0.06829\n","\n","Epoch 00073: val_loss improved from 0.06829 to 0.06704, saving model to ./model/73-0.0670.hdf5\n","\n","Epoch 00074: val_loss did not improve from 0.06704\n","\n","Epoch 00075: val_loss did not improve from 0.06704\n","\n","Epoch 00076: val_loss did not improve from 0.06704\n","\n","Epoch 00077: val_loss improved from 0.06704 to 0.06620, saving model to ./model/77-0.0662.hdf5\n","\n","Epoch 00078: val_loss improved from 0.06620 to 0.06596, saving model to ./model/78-0.0660.hdf5\n","\n","Epoch 00079: val_loss improved from 0.06596 to 0.06584, saving model to ./model/79-0.0658.hdf5\n","\n","Epoch 00080: val_loss did not improve from 0.06584\n","\n","Epoch 00081: val_loss did not improve from 0.06584\n","\n","Epoch 00082: val_loss did not improve from 0.06584\n","\n","Epoch 00083: val_loss improved from 0.06584 to 0.06325, saving model to ./model/83-0.0633.hdf5\n","\n","Epoch 00084: val_loss did not improve from 0.06325\n","\n","Epoch 00085: val_loss did not improve from 0.06325\n","\n","Epoch 00086: val_loss improved from 0.06325 to 0.06244, saving model to ./model/86-0.0624.hdf5\n","\n","Epoch 00087: val_loss did not improve from 0.06244\n","\n","Epoch 00088: val_loss did not improve from 0.06244\n","\n","Epoch 00089: val_loss did not improve from 0.06244\n","\n","Epoch 00090: val_loss did not improve from 0.06244\n","\n","Epoch 00091: val_loss did not improve from 0.06244\n","\n","Epoch 00092: val_loss did not improve from 0.06244\n","\n","Epoch 00093: val_loss did not improve from 0.06244\n","\n","Epoch 00094: val_loss did not improve from 0.06244\n","\n","Epoch 00095: val_loss improved from 0.06244 to 0.06095, saving model to ./model/95-0.0609.hdf5\n","\n","Epoch 00096: val_loss improved from 0.06095 to 0.06095, saving model to ./model/96-0.0609.hdf5\n","\n","Epoch 00097: val_loss did not improve from 0.06095\n","\n","Epoch 00098: val_loss did not improve from 0.06095\n","\n","Epoch 00099: val_loss did not improve from 0.06095\n","\n","Epoch 00100: val_loss improved from 0.06095 to 0.06036, saving model to ./model/100-0.0604.hdf5\n","\n","Epoch 00101: val_loss did not improve from 0.06036\n","\n","Epoch 00102: val_loss improved from 0.06036 to 0.05952, saving model to ./model/102-0.0595.hdf5\n","\n","Epoch 00103: val_loss did not improve from 0.05952\n","\n","Epoch 00104: val_loss did not improve from 0.05952\n","\n","Epoch 00105: val_loss did not improve from 0.05952\n","\n","Epoch 00106: val_loss did not improve from 0.05952\n","\n","Epoch 00107: val_loss did not improve from 0.05952\n","\n","Epoch 00108: val_loss improved from 0.05952 to 0.05921, saving model to ./model/108-0.0592.hdf5\n","\n","Epoch 00109: val_loss did not improve from 0.05921\n","\n","Epoch 00110: val_loss did not improve from 0.05921\n","\n","Epoch 00111: val_loss did not improve from 0.05921\n","\n","Epoch 00112: val_loss did not improve from 0.05921\n","\n","Epoch 00113: val_loss did not improve from 0.05921\n","\n","Epoch 00114: val_loss did not improve from 0.05921\n","\n","Epoch 00115: val_loss improved from 0.05921 to 0.05912, saving model to ./model/115-0.0591.hdf5\n","\n","Epoch 00116: val_loss did not improve from 0.05912\n","\n","Epoch 00117: val_loss did not improve from 0.05912\n","\n","Epoch 00118: val_loss improved from 0.05912 to 0.05725, saving model to ./model/118-0.0573.hdf5\n","\n","Epoch 00119: val_loss did not improve from 0.05725\n","\n","Epoch 00120: val_loss did not improve from 0.05725\n","\n","Epoch 00121: val_loss did not improve from 0.05725\n","\n","Epoch 00122: val_loss did not improve from 0.05725\n","\n","Epoch 00123: val_loss did not improve from 0.05725\n","\n","Epoch 00124: val_loss did not improve from 0.05725\n","\n","Epoch 00125: val_loss improved from 0.05725 to 0.05681, saving model to ./model/125-0.0568.hdf5\n","\n","Epoch 00126: val_loss did not improve from 0.05681\n","\n","Epoch 00127: val_loss did not improve from 0.05681\n","\n","Epoch 00128: val_loss did not improve from 0.05681\n","\n","Epoch 00129: val_loss did not improve from 0.05681\n","\n","Epoch 00130: val_loss did not improve from 0.05681\n","\n","Epoch 00131: val_loss improved from 0.05681 to 0.05653, saving model to ./model/131-0.0565.hdf5\n","\n","Epoch 00132: val_loss did not improve from 0.05653\n","\n","Epoch 00133: val_loss did not improve from 0.05653\n","\n","Epoch 00134: val_loss did not improve from 0.05653\n","\n","Epoch 00135: val_loss did not improve from 0.05653\n","\n","Epoch 00136: val_loss improved from 0.05653 to 0.05633, saving model to ./model/136-0.0563.hdf5\n","\n","Epoch 00137: val_loss improved from 0.05633 to 0.05509, saving model to ./model/137-0.0551.hdf5\n","\n","Epoch 00138: val_loss did not improve from 0.05509\n","\n","Epoch 00139: val_loss did not improve from 0.05509\n","\n","Epoch 00140: val_loss did not improve from 0.05509\n","\n","Epoch 00141: val_loss did not improve from 0.05509\n","\n","Epoch 00142: val_loss did not improve from 0.05509\n","\n","Epoch 00143: val_loss improved from 0.05509 to 0.05502, saving model to ./model/143-0.0550.hdf5\n","\n","Epoch 00144: val_loss did not improve from 0.05502\n","\n","Epoch 00145: val_loss did not improve from 0.05502\n","\n","Epoch 00146: val_loss improved from 0.05502 to 0.05464, saving model to ./model/146-0.0546.hdf5\n","\n","Epoch 00147: val_loss did not improve from 0.05464\n","\n","Epoch 00148: val_loss did not improve from 0.05464\n","\n","Epoch 00149: val_loss did not improve from 0.05464\n","\n","Epoch 00150: val_loss did not improve from 0.05464\n","\n","Epoch 00151: val_loss did not improve from 0.05464\n","\n","Epoch 00152: val_loss did not improve from 0.05464\n","\n","Epoch 00153: val_loss did not improve from 0.05464\n","\n","Epoch 00154: val_loss did not improve from 0.05464\n","\n","Epoch 00155: val_loss did not improve from 0.05464\n","\n","Epoch 00156: val_loss improved from 0.05464 to 0.05444, saving model to ./model/156-0.0544.hdf5\n","\n","Epoch 00157: val_loss did not improve from 0.05444\n","\n","Epoch 00158: val_loss did not improve from 0.05444\n","\n","Epoch 00159: val_loss did not improve from 0.05444\n","\n","Epoch 00160: val_loss improved from 0.05444 to 0.05377, saving model to ./model/160-0.0538.hdf5\n","\n","Epoch 00161: val_loss improved from 0.05377 to 0.05351, saving model to ./model/161-0.0535.hdf5\n","\n","Epoch 00162: val_loss did not improve from 0.05351\n","\n","Epoch 00163: val_loss did not improve from 0.05351\n","\n","Epoch 00164: val_loss did not improve from 0.05351\n","\n","Epoch 00165: val_loss did not improve from 0.05351\n","\n","Epoch 00166: val_loss did not improve from 0.05351\n","\n","Epoch 00167: val_loss did not improve from 0.05351\n","\n","Epoch 00168: val_loss did not improve from 0.05351\n","\n","Epoch 00169: val_loss did not improve from 0.05351\n","\n","Epoch 00170: val_loss did not improve from 0.05351\n","\n","Epoch 00171: val_loss improved from 0.05351 to 0.05314, saving model to ./model/171-0.0531.hdf5\n","\n","Epoch 00172: val_loss did not improve from 0.05314\n","\n","Epoch 00173: val_loss did not improve from 0.05314\n","\n","Epoch 00174: val_loss did not improve from 0.05314\n","\n","Epoch 00175: val_loss did not improve from 0.05314\n","\n","Epoch 00176: val_loss did not improve from 0.05314\n","\n","Epoch 00177: val_loss improved from 0.05314 to 0.05280, saving model to ./model/177-0.0528.hdf5\n","\n","Epoch 00178: val_loss did not improve from 0.05280\n","\n","Epoch 00179: val_loss did not improve from 0.05280\n","\n","Epoch 00180: val_loss did not improve from 0.05280\n","\n","Epoch 00181: val_loss did not improve from 0.05280\n","\n","Epoch 00182: val_loss did not improve from 0.05280\n","\n","Epoch 00183: val_loss did not improve from 0.05280\n","\n","Epoch 00184: val_loss did not improve from 0.05280\n","\n","Epoch 00185: val_loss did not improve from 0.05280\n","\n","Epoch 00186: val_loss did not improve from 0.05280\n","\n","Epoch 00187: val_loss did not improve from 0.05280\n","\n","Epoch 00188: val_loss did not improve from 0.05280\n","\n","Epoch 00189: val_loss did not improve from 0.05280\n","\n","Epoch 00190: val_loss improved from 0.05280 to 0.05262, saving model to ./model/190-0.0526.hdf5\n","\n","Epoch 00191: val_loss improved from 0.05262 to 0.05254, saving model to ./model/191-0.0525.hdf5\n","\n","Epoch 00192: val_loss did not improve from 0.05254\n","\n","Epoch 00193: val_loss improved from 0.05254 to 0.05242, saving model to ./model/193-0.0524.hdf5\n","\n","Epoch 00194: val_loss did not improve from 0.05242\n","\n","Epoch 00195: val_loss did not improve from 0.05242\n","\n","Epoch 00196: val_loss did not improve from 0.05242\n","\n","Epoch 00197: val_loss did not improve from 0.05242\n","\n","Epoch 00198: val_loss did not improve from 0.05242\n","\n","Epoch 00199: val_loss did not improve from 0.05242\n","\n","Epoch 00200: val_loss did not improve from 0.05242\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7fbbf0134f98>"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"92JPDC7DzfI0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594286175689,"user_tz":-540,"elapsed":39502,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":[""],"execution_count":10,"outputs":[]}]}