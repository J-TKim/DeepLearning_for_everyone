{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12_Wine_Check_and_Stop1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyORHnGhOcau2ms6/ft2eayf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"lOE2lIXpxTA6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594353017243,"user_tz":-540,"elapsed":2240,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}},"outputId":"2da3003f-c724-41e5-ccb2-f810a59587b7"},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import ModelCheckpoint, EarlyStopping"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"qLx3HpD6xkqV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594353017248,"user_tz":-540,"elapsed":2208,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"fP8pVEMRxqRE","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594353017252,"user_tz":-540,"elapsed":2172,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["# seed값 설정\n","seed = 3\n","\n","np.random.seed(seed)\n","tf.random.set_seed(seed)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"vOnn7iQ4xv1S","colab_type":"code","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1594353053953,"user_tz":-540,"elapsed":38841,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}},"outputId":"9fe5475a-d5e3-4fa9-da05-d896446d06ba"},"source":["from google.colab import files \n","\n","Uploaded = files.upload()\n","df_pre = pd.read_csv(\"wine.csv\", header = None)"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-3ec1379d-9beb-48ee-bb34-deeb76754d83\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3ec1379d-9beb-48ee-bb34-deeb76754d83\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving wine.csv to wine (1).csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"If6m5q2AyG3u","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594353053957,"user_tz":-540,"elapsed":38822,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["df = df_pre.sample(frac = 0.15)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"DaY_KpOYyLqg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594353053959,"user_tz":-540,"elapsed":38809,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["dataset = df.values\n","\n","X = dataset[:, 0:12]\n","Y = dataset[:, 12]"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"NhfEqUdFyRaR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594353054892,"user_tz":-540,"elapsed":39729,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["model = Sequential()\n","model.add(Dense(30, input_dim = 12, activation= \"relu\"))\n","model.add(Dense(12, activation= \"relu\"))\n","model.add(Dense(8, activation=\"relu\"))\n","model.add(Dense(1, activation=\"sigmoid\"))"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"KR6PB_gwyhEv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594353054895,"user_tz":-540,"elapsed":39719,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["model.compile(loss = \"binary_crossentropy\",\n","              optimizer = \"adam\",\n","              metrics = [\"accuracy\"])"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"eS5mzSeIy36N","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594353054898,"user_tz":-540,"elapsed":39710,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["# 모델 저장 폴더 만들기\n","MODEL_DIR = \"./model/\"\n","if not os.path.exists(MODEL_DIR):\n","  os.mkdir(MODEL_DIR)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5uJ7moUzGBA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594353331909,"user_tz":-540,"elapsed":929,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["modelpath = \"./model/{epoch:.4f}-{val_loss:.4f}.hdf5\""],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"hpWL4Uz0zORL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594353333036,"user_tz":-540,"elapsed":836,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["# 모델 업데이트 및 저장\n","checkpointer = ModelCheckpoint(filepath=modelpath, monitor = \"val_loss\",\n","                               verbose = 1, save_best_only = True)"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sl5W2cqUzbGO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594353333907,"user_tz":-540,"elapsed":1532,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":["# 학습 자동 중단 설정\n","early_stoppint_callback = EarlyStopping(monitor=\"val_loss\",\n","                                        patience = 100)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"wmqWo_d_zi9r","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594353346488,"user_tz":-540,"elapsed":13994,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}},"outputId":"83263eb5-6065-4115-bb4b-5bd9a21ef96b"},"source":["model.fit(X, Y, validation_split= 0.2, epochs = 3500, batch_size= 500, verbose = 0, callbacks=[early_stoppint_callback, checkpointer])"],"execution_count":23,"outputs":[{"output_type":"stream","text":["\n","Epoch 00001: val_loss improved from inf to 0.36557, saving model to ./model/1.0000-0.3656.hdf5\n","\n","Epoch 00002: val_loss improved from 0.36557 to 0.35252, saving model to ./model/2.0000-0.3525.hdf5\n","\n","Epoch 00003: val_loss did not improve from 0.35252\n","\n","Epoch 00004: val_loss did not improve from 0.35252\n","\n","Epoch 00005: val_loss did not improve from 0.35252\n","\n","Epoch 00006: val_loss did not improve from 0.35252\n","\n","Epoch 00007: val_loss did not improve from 0.35252\n","\n","Epoch 00008: val_loss improved from 0.35252 to 0.33442, saving model to ./model/8.0000-0.3344.hdf5\n","\n","Epoch 00009: val_loss improved from 0.33442 to 0.31363, saving model to ./model/9.0000-0.3136.hdf5\n","\n","Epoch 00010: val_loss improved from 0.31363 to 0.30246, saving model to ./model/10.0000-0.3025.hdf5\n","\n","Epoch 00011: val_loss improved from 0.30246 to 0.29804, saving model to ./model/11.0000-0.2980.hdf5\n","\n","Epoch 00012: val_loss improved from 0.29804 to 0.29571, saving model to ./model/12.0000-0.2957.hdf5\n","\n","Epoch 00013: val_loss improved from 0.29571 to 0.29346, saving model to ./model/13.0000-0.2935.hdf5\n","\n","Epoch 00014: val_loss improved from 0.29346 to 0.29090, saving model to ./model/14.0000-0.2909.hdf5\n","\n","Epoch 00015: val_loss improved from 0.29090 to 0.28867, saving model to ./model/15.0000-0.2887.hdf5\n","\n","Epoch 00016: val_loss improved from 0.28867 to 0.28771, saving model to ./model/16.0000-0.2877.hdf5\n","\n","Epoch 00017: val_loss did not improve from 0.28771\n","\n","Epoch 00018: val_loss did not improve from 0.28771\n","\n","Epoch 00019: val_loss did not improve from 0.28771\n","\n","Epoch 00020: val_loss did not improve from 0.28771\n","\n","Epoch 00021: val_loss did not improve from 0.28771\n","\n","Epoch 00022: val_loss improved from 0.28771 to 0.28672, saving model to ./model/22.0000-0.2867.hdf5\n","\n","Epoch 00023: val_loss improved from 0.28672 to 0.28237, saving model to ./model/23.0000-0.2824.hdf5\n","\n","Epoch 00024: val_loss improved from 0.28237 to 0.27857, saving model to ./model/24.0000-0.2786.hdf5\n","\n","Epoch 00025: val_loss improved from 0.27857 to 0.27540, saving model to ./model/25.0000-0.2754.hdf5\n","\n","Epoch 00026: val_loss improved from 0.27540 to 0.27325, saving model to ./model/26.0000-0.2732.hdf5\n","\n","Epoch 00027: val_loss improved from 0.27325 to 0.27192, saving model to ./model/27.0000-0.2719.hdf5\n","\n","Epoch 00028: val_loss improved from 0.27192 to 0.27105, saving model to ./model/28.0000-0.2711.hdf5\n","\n","Epoch 00029: val_loss did not improve from 0.27105\n","\n","Epoch 00030: val_loss did not improve from 0.27105\n","\n","Epoch 00031: val_loss did not improve from 0.27105\n","\n","Epoch 00032: val_loss improved from 0.27105 to 0.26980, saving model to ./model/32.0000-0.2698.hdf5\n","\n","Epoch 00033: val_loss improved from 0.26980 to 0.26767, saving model to ./model/33.0000-0.2677.hdf5\n","\n","Epoch 00034: val_loss improved from 0.26767 to 0.26528, saving model to ./model/34.0000-0.2653.hdf5\n","\n","Epoch 00035: val_loss improved from 0.26528 to 0.26260, saving model to ./model/35.0000-0.2626.hdf5\n","\n","Epoch 00036: val_loss improved from 0.26260 to 0.26002, saving model to ./model/36.0000-0.2600.hdf5\n","\n","Epoch 00037: val_loss improved from 0.26002 to 0.25778, saving model to ./model/37.0000-0.2578.hdf5\n","\n","Epoch 00038: val_loss improved from 0.25778 to 0.25636, saving model to ./model/38.0000-0.2564.hdf5\n","\n","Epoch 00039: val_loss improved from 0.25636 to 0.25530, saving model to ./model/39.0000-0.2553.hdf5\n","\n","Epoch 00040: val_loss improved from 0.25530 to 0.25445, saving model to ./model/40.0000-0.2545.hdf5\n","\n","Epoch 00041: val_loss improved from 0.25445 to 0.25371, saving model to ./model/41.0000-0.2537.hdf5\n","\n","Epoch 00042: val_loss improved from 0.25371 to 0.25250, saving model to ./model/42.0000-0.2525.hdf5\n","\n","Epoch 00043: val_loss improved from 0.25250 to 0.25150, saving model to ./model/43.0000-0.2515.hdf5\n","\n","Epoch 00044: val_loss improved from 0.25150 to 0.24986, saving model to ./model/44.0000-0.2499.hdf5\n","\n","Epoch 00045: val_loss improved from 0.24986 to 0.24846, saving model to ./model/45.0000-0.2485.hdf5\n","\n","Epoch 00046: val_loss improved from 0.24846 to 0.24733, saving model to ./model/46.0000-0.2473.hdf5\n","\n","Epoch 00047: val_loss improved from 0.24733 to 0.24543, saving model to ./model/47.0000-0.2454.hdf5\n","\n","Epoch 00048: val_loss improved from 0.24543 to 0.24318, saving model to ./model/48.0000-0.2432.hdf5\n","\n","Epoch 00049: val_loss improved from 0.24318 to 0.24105, saving model to ./model/49.0000-0.2410.hdf5\n","\n","Epoch 00050: val_loss improved from 0.24105 to 0.23951, saving model to ./model/50.0000-0.2395.hdf5\n","\n","Epoch 00051: val_loss improved from 0.23951 to 0.23863, saving model to ./model/51.0000-0.2386.hdf5\n","\n","Epoch 00052: val_loss improved from 0.23863 to 0.23843, saving model to ./model/52.0000-0.2384.hdf5\n","\n","Epoch 00053: val_loss improved from 0.23843 to 0.23775, saving model to ./model/53.0000-0.2378.hdf5\n","\n","Epoch 00054: val_loss improved from 0.23775 to 0.23688, saving model to ./model/54.0000-0.2369.hdf5\n","\n","Epoch 00055: val_loss improved from 0.23688 to 0.23536, saving model to ./model/55.0000-0.2354.hdf5\n","\n","Epoch 00056: val_loss improved from 0.23536 to 0.23344, saving model to ./model/56.0000-0.2334.hdf5\n","\n","Epoch 00057: val_loss improved from 0.23344 to 0.23071, saving model to ./model/57.0000-0.2307.hdf5\n","\n","Epoch 00058: val_loss improved from 0.23071 to 0.22795, saving model to ./model/58.0000-0.2280.hdf5\n","\n","Epoch 00059: val_loss improved from 0.22795 to 0.22621, saving model to ./model/59.0000-0.2262.hdf5\n","\n","Epoch 00060: val_loss improved from 0.22621 to 0.22385, saving model to ./model/60.0000-0.2239.hdf5\n","\n","Epoch 00061: val_loss improved from 0.22385 to 0.22179, saving model to ./model/61.0000-0.2218.hdf5\n","\n","Epoch 00062: val_loss improved from 0.22179 to 0.22113, saving model to ./model/62.0000-0.2211.hdf5\n","\n","Epoch 00063: val_loss did not improve from 0.22113\n","\n","Epoch 00064: val_loss improved from 0.22113 to 0.22113, saving model to ./model/64.0000-0.2211.hdf5\n","\n","Epoch 00065: val_loss improved from 0.22113 to 0.22044, saving model to ./model/65.0000-0.2204.hdf5\n","\n","Epoch 00066: val_loss improved from 0.22044 to 0.21878, saving model to ./model/66.0000-0.2188.hdf5\n","\n","Epoch 00067: val_loss improved from 0.21878 to 0.21646, saving model to ./model/67.0000-0.2165.hdf5\n","\n","Epoch 00068: val_loss improved from 0.21646 to 0.21383, saving model to ./model/68.0000-0.2138.hdf5\n","\n","Epoch 00069: val_loss improved from 0.21383 to 0.21174, saving model to ./model/69.0000-0.2117.hdf5\n","\n","Epoch 00070: val_loss improved from 0.21174 to 0.20977, saving model to ./model/70.0000-0.2098.hdf5\n","\n","Epoch 00071: val_loss improved from 0.20977 to 0.20921, saving model to ./model/71.0000-0.2092.hdf5\n","\n","Epoch 00072: val_loss did not improve from 0.20921\n","\n","Epoch 00073: val_loss improved from 0.20921 to 0.20812, saving model to ./model/73.0000-0.2081.hdf5\n","\n","Epoch 00074: val_loss improved from 0.20812 to 0.20610, saving model to ./model/74.0000-0.2061.hdf5\n","\n","Epoch 00075: val_loss improved from 0.20610 to 0.20474, saving model to ./model/75.0000-0.2047.hdf5\n","\n","Epoch 00076: val_loss improved from 0.20474 to 0.20359, saving model to ./model/76.0000-0.2036.hdf5\n","\n","Epoch 00077: val_loss improved from 0.20359 to 0.20284, saving model to ./model/77.0000-0.2028.hdf5\n","\n","Epoch 00078: val_loss improved from 0.20284 to 0.20237, saving model to ./model/78.0000-0.2024.hdf5\n","\n","Epoch 00079: val_loss improved from 0.20237 to 0.20227, saving model to ./model/79.0000-0.2023.hdf5\n","\n","Epoch 00080: val_loss improved from 0.20227 to 0.20106, saving model to ./model/80.0000-0.2011.hdf5\n","\n","Epoch 00081: val_loss improved from 0.20106 to 0.19981, saving model to ./model/81.0000-0.1998.hdf5\n","\n","Epoch 00082: val_loss improved from 0.19981 to 0.19731, saving model to ./model/82.0000-0.1973.hdf5\n","\n","Epoch 00083: val_loss improved from 0.19731 to 0.19548, saving model to ./model/83.0000-0.1955.hdf5\n","\n","Epoch 00084: val_loss improved from 0.19548 to 0.19304, saving model to ./model/84.0000-0.1930.hdf5\n","\n","Epoch 00085: val_loss improved from 0.19304 to 0.19143, saving model to ./model/85.0000-0.1914.hdf5\n","\n","Epoch 00086: val_loss improved from 0.19143 to 0.19101, saving model to ./model/86.0000-0.1910.hdf5\n","\n","Epoch 00087: val_loss did not improve from 0.19101\n","\n","Epoch 00088: val_loss did not improve from 0.19101\n","\n","Epoch 00089: val_loss did not improve from 0.19101\n","\n","Epoch 00090: val_loss improved from 0.19101 to 0.18973, saving model to ./model/90.0000-0.1897.hdf5\n","\n","Epoch 00091: val_loss improved from 0.18973 to 0.18630, saving model to ./model/91.0000-0.1863.hdf5\n","\n","Epoch 00092: val_loss improved from 0.18630 to 0.18417, saving model to ./model/92.0000-0.1842.hdf5\n","\n","Epoch 00093: val_loss improved from 0.18417 to 0.18330, saving model to ./model/93.0000-0.1833.hdf5\n","\n","Epoch 00094: val_loss improved from 0.18330 to 0.18315, saving model to ./model/94.0000-0.1832.hdf5\n","\n","Epoch 00095: val_loss improved from 0.18315 to 0.18255, saving model to ./model/95.0000-0.1826.hdf5\n","\n","Epoch 00096: val_loss improved from 0.18255 to 0.18151, saving model to ./model/96.0000-0.1815.hdf5\n","\n","Epoch 00097: val_loss did not improve from 0.18151\n","\n","Epoch 00098: val_loss improved from 0.18151 to 0.17983, saving model to ./model/98.0000-0.1798.hdf5\n","\n","Epoch 00099: val_loss improved from 0.17983 to 0.17966, saving model to ./model/99.0000-0.1797.hdf5\n","\n","Epoch 00100: val_loss improved from 0.17966 to 0.17791, saving model to ./model/100.0000-0.1779.hdf5\n","\n","Epoch 00101: val_loss improved from 0.17791 to 0.17652, saving model to ./model/101.0000-0.1765.hdf5\n","\n","Epoch 00102: val_loss improved from 0.17652 to 0.17494, saving model to ./model/102.0000-0.1749.hdf5\n","\n","Epoch 00103: val_loss improved from 0.17494 to 0.17452, saving model to ./model/103.0000-0.1745.hdf5\n","\n","Epoch 00104: val_loss did not improve from 0.17452\n","\n","Epoch 00105: val_loss did not improve from 0.17452\n","\n","Epoch 00106: val_loss did not improve from 0.17452\n","\n","Epoch 00107: val_loss did not improve from 0.17452\n","\n","Epoch 00108: val_loss improved from 0.17452 to 0.17307, saving model to ./model/108.0000-0.1731.hdf5\n","\n","Epoch 00109: val_loss improved from 0.17307 to 0.17028, saving model to ./model/109.0000-0.1703.hdf5\n","\n","Epoch 00110: val_loss did not improve from 0.17028\n","\n","Epoch 00111: val_loss did not improve from 0.17028\n","\n","Epoch 00112: val_loss did not improve from 0.17028\n","\n","Epoch 00113: val_loss did not improve from 0.17028\n","\n","Epoch 00114: val_loss did not improve from 0.17028\n","\n","Epoch 00115: val_loss improved from 0.17028 to 0.16643, saving model to ./model/115.0000-0.1664.hdf5\n","\n","Epoch 00116: val_loss improved from 0.16643 to 0.16400, saving model to ./model/116.0000-0.1640.hdf5\n","\n","Epoch 00117: val_loss improved from 0.16400 to 0.16385, saving model to ./model/117.0000-0.1639.hdf5\n","\n","Epoch 00118: val_loss did not improve from 0.16385\n","\n","Epoch 00119: val_loss did not improve from 0.16385\n","\n","Epoch 00120: val_loss did not improve from 0.16385\n","\n","Epoch 00121: val_loss did not improve from 0.16385\n","\n","Epoch 00122: val_loss improved from 0.16385 to 0.16347, saving model to ./model/122.0000-0.1635.hdf5\n","\n","Epoch 00123: val_loss improved from 0.16347 to 0.16252, saving model to ./model/123.0000-0.1625.hdf5\n","\n","Epoch 00124: val_loss improved from 0.16252 to 0.16152, saving model to ./model/124.0000-0.1615.hdf5\n","\n","Epoch 00125: val_loss did not improve from 0.16152\n","\n","Epoch 00126: val_loss did not improve from 0.16152\n","\n","Epoch 00127: val_loss did not improve from 0.16152\n","\n","Epoch 00128: val_loss did not improve from 0.16152\n","\n","Epoch 00129: val_loss improved from 0.16152 to 0.16101, saving model to ./model/129.0000-0.1610.hdf5\n","\n","Epoch 00130: val_loss improved from 0.16101 to 0.15943, saving model to ./model/130.0000-0.1594.hdf5\n","\n","Epoch 00131: val_loss did not improve from 0.15943\n","\n","Epoch 00132: val_loss did not improve from 0.15943\n","\n","Epoch 00133: val_loss did not improve from 0.15943\n","\n","Epoch 00134: val_loss did not improve from 0.15943\n","\n","Epoch 00135: val_loss did not improve from 0.15943\n","\n","Epoch 00136: val_loss improved from 0.15943 to 0.15847, saving model to ./model/136.0000-0.1585.hdf5\n","\n","Epoch 00137: val_loss improved from 0.15847 to 0.15736, saving model to ./model/137.0000-0.1574.hdf5\n","\n","Epoch 00138: val_loss did not improve from 0.15736\n","\n","Epoch 00139: val_loss did not improve from 0.15736\n","\n","Epoch 00140: val_loss did not improve from 0.15736\n","\n","Epoch 00141: val_loss did not improve from 0.15736\n","\n","Epoch 00142: val_loss improved from 0.15736 to 0.15718, saving model to ./model/142.0000-0.1572.hdf5\n","\n","Epoch 00143: val_loss improved from 0.15718 to 0.15418, saving model to ./model/143.0000-0.1542.hdf5\n","\n","Epoch 00144: val_loss improved from 0.15418 to 0.15413, saving model to ./model/144.0000-0.1541.hdf5\n","\n","Epoch 00145: val_loss did not improve from 0.15413\n","\n","Epoch 00146: val_loss did not improve from 0.15413\n","\n","Epoch 00147: val_loss did not improve from 0.15413\n","\n","Epoch 00148: val_loss improved from 0.15413 to 0.15408, saving model to ./model/148.0000-0.1541.hdf5\n","\n","Epoch 00149: val_loss improved from 0.15408 to 0.15268, saving model to ./model/149.0000-0.1527.hdf5\n","\n","Epoch 00150: val_loss improved from 0.15268 to 0.15245, saving model to ./model/150.0000-0.1525.hdf5\n","\n","Epoch 00151: val_loss did not improve from 0.15245\n","\n","Epoch 00152: val_loss did not improve from 0.15245\n","\n","Epoch 00153: val_loss did not improve from 0.15245\n","\n","Epoch 00154: val_loss did not improve from 0.15245\n","\n","Epoch 00155: val_loss improved from 0.15245 to 0.15096, saving model to ./model/155.0000-0.1510.hdf5\n","\n","Epoch 00156: val_loss improved from 0.15096 to 0.15030, saving model to ./model/156.0000-0.1503.hdf5\n","\n","Epoch 00157: val_loss did not improve from 0.15030\n","\n","Epoch 00158: val_loss did not improve from 0.15030\n","\n","Epoch 00159: val_loss did not improve from 0.15030\n","\n","Epoch 00160: val_loss did not improve from 0.15030\n","\n","Epoch 00161: val_loss improved from 0.15030 to 0.14743, saving model to ./model/161.0000-0.1474.hdf5\n","\n","Epoch 00162: val_loss improved from 0.14743 to 0.14585, saving model to ./model/162.0000-0.1459.hdf5\n","\n","Epoch 00163: val_loss did not improve from 0.14585\n","\n","Epoch 00164: val_loss did not improve from 0.14585\n","\n","Epoch 00165: val_loss did not improve from 0.14585\n","\n","Epoch 00166: val_loss did not improve from 0.14585\n","\n","Epoch 00167: val_loss did not improve from 0.14585\n","\n","Epoch 00168: val_loss did not improve from 0.14585\n","\n","Epoch 00169: val_loss did not improve from 0.14585\n","\n","Epoch 00170: val_loss did not improve from 0.14585\n","\n","Epoch 00171: val_loss improved from 0.14585 to 0.14518, saving model to ./model/171.0000-0.1452.hdf5\n","\n","Epoch 00172: val_loss improved from 0.14518 to 0.14453, saving model to ./model/172.0000-0.1445.hdf5\n","\n","Epoch 00173: val_loss did not improve from 0.14453\n","\n","Epoch 00174: val_loss did not improve from 0.14453\n","\n","Epoch 00175: val_loss did not improve from 0.14453\n","\n","Epoch 00176: val_loss did not improve from 0.14453\n","\n","Epoch 00177: val_loss improved from 0.14453 to 0.14343, saving model to ./model/177.0000-0.1434.hdf5\n","\n","Epoch 00178: val_loss improved from 0.14343 to 0.14140, saving model to ./model/178.0000-0.1414.hdf5\n","\n","Epoch 00179: val_loss did not improve from 0.14140\n","\n","Epoch 00180: val_loss did not improve from 0.14140\n","\n","Epoch 00181: val_loss did not improve from 0.14140\n","\n","Epoch 00182: val_loss did not improve from 0.14140\n","\n","Epoch 00183: val_loss did not improve from 0.14140\n","\n","Epoch 00184: val_loss improved from 0.14140 to 0.13965, saving model to ./model/184.0000-0.1397.hdf5\n","\n","Epoch 00185: val_loss did not improve from 0.13965\n","\n","Epoch 00186: val_loss did not improve from 0.13965\n","\n","Epoch 00187: val_loss did not improve from 0.13965\n","\n","Epoch 00188: val_loss did not improve from 0.13965\n","\n","Epoch 00189: val_loss did not improve from 0.13965\n","\n","Epoch 00190: val_loss did not improve from 0.13965\n","\n","Epoch 00191: val_loss did not improve from 0.13965\n","\n","Epoch 00192: val_loss did not improve from 0.13965\n","\n","Epoch 00193: val_loss did not improve from 0.13965\n","\n","Epoch 00194: val_loss did not improve from 0.13965\n","\n","Epoch 00195: val_loss did not improve from 0.13965\n","\n","Epoch 00196: val_loss did not improve from 0.13965\n","\n","Epoch 00197: val_loss did not improve from 0.13965\n","\n","Epoch 00198: val_loss did not improve from 0.13965\n","\n","Epoch 00199: val_loss did not improve from 0.13965\n","\n","Epoch 00200: val_loss improved from 0.13965 to 0.13738, saving model to ./model/200.0000-0.1374.hdf5\n","\n","Epoch 00201: val_loss improved from 0.13738 to 0.13664, saving model to ./model/201.0000-0.1366.hdf5\n","\n","Epoch 00202: val_loss did not improve from 0.13664\n","\n","Epoch 00203: val_loss did not improve from 0.13664\n","\n","Epoch 00204: val_loss improved from 0.13664 to 0.13576, saving model to ./model/204.0000-0.1358.hdf5\n","\n","Epoch 00205: val_loss did not improve from 0.13576\n","\n","Epoch 00206: val_loss did not improve from 0.13576\n","\n","Epoch 00207: val_loss did not improve from 0.13576\n","\n","Epoch 00208: val_loss did not improve from 0.13576\n","\n","Epoch 00209: val_loss improved from 0.13576 to 0.13474, saving model to ./model/209.0000-0.1347.hdf5\n","\n","Epoch 00210: val_loss improved from 0.13474 to 0.13345, saving model to ./model/210.0000-0.1334.hdf5\n","\n","Epoch 00211: val_loss did not improve from 0.13345\n","\n","Epoch 00212: val_loss did not improve from 0.13345\n","\n","Epoch 00213: val_loss did not improve from 0.13345\n","\n","Epoch 00214: val_loss did not improve from 0.13345\n","\n","Epoch 00215: val_loss improved from 0.13345 to 0.13291, saving model to ./model/215.0000-0.1329.hdf5\n","\n","Epoch 00216: val_loss did not improve from 0.13291\n","\n","Epoch 00217: val_loss did not improve from 0.13291\n","\n","Epoch 00218: val_loss did not improve from 0.13291\n","\n","Epoch 00219: val_loss improved from 0.13291 to 0.13010, saving model to ./model/219.0000-0.1301.hdf5\n","\n","Epoch 00220: val_loss improved from 0.13010 to 0.12982, saving model to ./model/220.0000-0.1298.hdf5\n","\n","Epoch 00221: val_loss did not improve from 0.12982\n","\n","Epoch 00222: val_loss did not improve from 0.12982\n","\n","Epoch 00223: val_loss did not improve from 0.12982\n","\n","Epoch 00224: val_loss improved from 0.12982 to 0.12812, saving model to ./model/224.0000-0.1281.hdf5\n","\n","Epoch 00225: val_loss did not improve from 0.12812\n","\n","Epoch 00226: val_loss did not improve from 0.12812\n","\n","Epoch 00227: val_loss did not improve from 0.12812\n","\n","Epoch 00228: val_loss did not improve from 0.12812\n","\n","Epoch 00229: val_loss improved from 0.12812 to 0.12770, saving model to ./model/229.0000-0.1277.hdf5\n","\n","Epoch 00230: val_loss improved from 0.12770 to 0.12600, saving model to ./model/230.0000-0.1260.hdf5\n","\n","Epoch 00231: val_loss did not improve from 0.12600\n","\n","Epoch 00232: val_loss did not improve from 0.12600\n","\n","Epoch 00233: val_loss did not improve from 0.12600\n","\n","Epoch 00234: val_loss improved from 0.12600 to 0.12514, saving model to ./model/234.0000-0.1251.hdf5\n","\n","Epoch 00235: val_loss improved from 0.12514 to 0.12450, saving model to ./model/235.0000-0.1245.hdf5\n","\n","Epoch 00236: val_loss did not improve from 0.12450\n","\n","Epoch 00237: val_loss did not improve from 0.12450\n","\n","Epoch 00238: val_loss did not improve from 0.12450\n","\n","Epoch 00239: val_loss did not improve from 0.12450\n","\n","Epoch 00240: val_loss improved from 0.12450 to 0.12161, saving model to ./model/240.0000-0.1216.hdf5\n","\n","Epoch 00241: val_loss did not improve from 0.12161\n","\n","Epoch 00242: val_loss did not improve from 0.12161\n","\n","Epoch 00243: val_loss did not improve from 0.12161\n","\n","Epoch 00244: val_loss did not improve from 0.12161\n","\n","Epoch 00245: val_loss did not improve from 0.12161\n","\n","Epoch 00246: val_loss improved from 0.12161 to 0.12103, saving model to ./model/246.0000-0.1210.hdf5\n","\n","Epoch 00247: val_loss did not improve from 0.12103\n","\n","Epoch 00248: val_loss did not improve from 0.12103\n","\n","Epoch 00249: val_loss did not improve from 0.12103\n","\n","Epoch 00250: val_loss improved from 0.12103 to 0.12078, saving model to ./model/250.0000-0.1208.hdf5\n","\n","Epoch 00251: val_loss did not improve from 0.12078\n","\n","Epoch 00252: val_loss did not improve from 0.12078\n","\n","Epoch 00253: val_loss did not improve from 0.12078\n","\n","Epoch 00254: val_loss did not improve from 0.12078\n","\n","Epoch 00255: val_loss did not improve from 0.12078\n","\n","Epoch 00256: val_loss did not improve from 0.12078\n","\n","Epoch 00257: val_loss did not improve from 0.12078\n","\n","Epoch 00258: val_loss did not improve from 0.12078\n","\n","Epoch 00259: val_loss improved from 0.12078 to 0.11910, saving model to ./model/259.0000-0.1191.hdf5\n","\n","Epoch 00260: val_loss did not improve from 0.11910\n","\n","Epoch 00261: val_loss did not improve from 0.11910\n","\n","Epoch 00262: val_loss did not improve from 0.11910\n","\n","Epoch 00263: val_loss improved from 0.11910 to 0.11902, saving model to ./model/263.0000-0.1190.hdf5\n","\n","Epoch 00264: val_loss did not improve from 0.11902\n","\n","Epoch 00265: val_loss did not improve from 0.11902\n","\n","Epoch 00266: val_loss did not improve from 0.11902\n","\n","Epoch 00267: val_loss did not improve from 0.11902\n","\n","Epoch 00268: val_loss did not improve from 0.11902\n","\n","Epoch 00269: val_loss did not improve from 0.11902\n","\n","Epoch 00270: val_loss did not improve from 0.11902\n","\n","Epoch 00271: val_loss did not improve from 0.11902\n","\n","Epoch 00272: val_loss did not improve from 0.11902\n","\n","Epoch 00273: val_loss did not improve from 0.11902\n","\n","Epoch 00274: val_loss did not improve from 0.11902\n","\n","Epoch 00275: val_loss improved from 0.11902 to 0.11596, saving model to ./model/275.0000-0.1160.hdf5\n","\n","Epoch 00276: val_loss did not improve from 0.11596\n","\n","Epoch 00277: val_loss did not improve from 0.11596\n","\n","Epoch 00278: val_loss did not improve from 0.11596\n","\n","Epoch 00279: val_loss did not improve from 0.11596\n","\n","Epoch 00280: val_loss improved from 0.11596 to 0.11385, saving model to ./model/280.0000-0.1139.hdf5\n","\n","Epoch 00281: val_loss did not improve from 0.11385\n","\n","Epoch 00282: val_loss did not improve from 0.11385\n","\n","Epoch 00283: val_loss did not improve from 0.11385\n","\n","Epoch 00284: val_loss improved from 0.11385 to 0.11364, saving model to ./model/284.0000-0.1136.hdf5\n","\n","Epoch 00285: val_loss did not improve from 0.11364\n","\n","Epoch 00286: val_loss did not improve from 0.11364\n","\n","Epoch 00287: val_loss did not improve from 0.11364\n","\n","Epoch 00288: val_loss did not improve from 0.11364\n","\n","Epoch 00289: val_loss did not improve from 0.11364\n","\n","Epoch 00290: val_loss did not improve from 0.11364\n","\n","Epoch 00291: val_loss did not improve from 0.11364\n","\n","Epoch 00292: val_loss did not improve from 0.11364\n","\n","Epoch 00293: val_loss did not improve from 0.11364\n","\n","Epoch 00294: val_loss did not improve from 0.11364\n","\n","Epoch 00295: val_loss did not improve from 0.11364\n","\n","Epoch 00296: val_loss did not improve from 0.11364\n","\n","Epoch 00297: val_loss did not improve from 0.11364\n","\n","Epoch 00298: val_loss did not improve from 0.11364\n","\n","Epoch 00299: val_loss did not improve from 0.11364\n","\n","Epoch 00300: val_loss did not improve from 0.11364\n","\n","Epoch 00301: val_loss improved from 0.11364 to 0.11352, saving model to ./model/301.0000-0.1135.hdf5\n","\n","Epoch 00302: val_loss did not improve from 0.11352\n","\n","Epoch 00303: val_loss did not improve from 0.11352\n","\n","Epoch 00304: val_loss did not improve from 0.11352\n","\n","Epoch 00305: val_loss improved from 0.11352 to 0.11345, saving model to ./model/305.0000-0.1134.hdf5\n","\n","Epoch 00306: val_loss improved from 0.11345 to 0.11301, saving model to ./model/306.0000-0.1130.hdf5\n","\n","Epoch 00307: val_loss did not improve from 0.11301\n","\n","Epoch 00308: val_loss did not improve from 0.11301\n","\n","Epoch 00309: val_loss did not improve from 0.11301\n","\n","Epoch 00310: val_loss improved from 0.11301 to 0.11185, saving model to ./model/310.0000-0.1118.hdf5\n","\n","Epoch 00311: val_loss did not improve from 0.11185\n","\n","Epoch 00312: val_loss did not improve from 0.11185\n","\n","Epoch 00313: val_loss improved from 0.11185 to 0.11038, saving model to ./model/313.0000-0.1104.hdf5\n","\n","Epoch 00314: val_loss did not improve from 0.11038\n","\n","Epoch 00315: val_loss did not improve from 0.11038\n","\n","Epoch 00316: val_loss did not improve from 0.11038\n","\n","Epoch 00317: val_loss did not improve from 0.11038\n","\n","Epoch 00318: val_loss improved from 0.11038 to 0.11003, saving model to ./model/318.0000-0.1100.hdf5\n","\n","Epoch 00319: val_loss did not improve from 0.11003\n","\n","Epoch 00320: val_loss did not improve from 0.11003\n","\n","Epoch 00321: val_loss did not improve from 0.11003\n","\n","Epoch 00322: val_loss did not improve from 0.11003\n","\n","Epoch 00323: val_loss did not improve from 0.11003\n","\n","Epoch 00324: val_loss did not improve from 0.11003\n","\n","Epoch 00325: val_loss did not improve from 0.11003\n","\n","Epoch 00326: val_loss improved from 0.11003 to 0.10924, saving model to ./model/326.0000-0.1092.hdf5\n","\n","Epoch 00327: val_loss improved from 0.10924 to 0.10726, saving model to ./model/327.0000-0.1073.hdf5\n","\n","Epoch 00328: val_loss did not improve from 0.10726\n","\n","Epoch 00329: val_loss did not improve from 0.10726\n","\n","Epoch 00330: val_loss did not improve from 0.10726\n","\n","Epoch 00331: val_loss improved from 0.10726 to 0.10606, saving model to ./model/331.0000-0.1061.hdf5\n","\n","Epoch 00332: val_loss did not improve from 0.10606\n","\n","Epoch 00333: val_loss did not improve from 0.10606\n","\n","Epoch 00334: val_loss did not improve from 0.10606\n","\n","Epoch 00335: val_loss did not improve from 0.10606\n","\n","Epoch 00336: val_loss did not improve from 0.10606\n","\n","Epoch 00337: val_loss did not improve from 0.10606\n","\n","Epoch 00338: val_loss did not improve from 0.10606\n","\n","Epoch 00339: val_loss did not improve from 0.10606\n","\n","Epoch 00340: val_loss improved from 0.10606 to 0.10522, saving model to ./model/340.0000-0.1052.hdf5\n","\n","Epoch 00341: val_loss did not improve from 0.10522\n","\n","Epoch 00342: val_loss did not improve from 0.10522\n","\n","Epoch 00343: val_loss did not improve from 0.10522\n","\n","Epoch 00344: val_loss improved from 0.10522 to 0.10521, saving model to ./model/344.0000-0.1052.hdf5\n","\n","Epoch 00345: val_loss did not improve from 0.10521\n","\n","Epoch 00346: val_loss did not improve from 0.10521\n","\n","Epoch 00347: val_loss did not improve from 0.10521\n","\n","Epoch 00348: val_loss improved from 0.10521 to 0.10349, saving model to ./model/348.0000-0.1035.hdf5\n","\n","Epoch 00349: val_loss did not improve from 0.10349\n","\n","Epoch 00350: val_loss did not improve from 0.10349\n","\n","Epoch 00351: val_loss did not improve from 0.10349\n","\n","Epoch 00352: val_loss did not improve from 0.10349\n","\n","Epoch 00353: val_loss did not improve from 0.10349\n","\n","Epoch 00354: val_loss did not improve from 0.10349\n","\n","Epoch 00355: val_loss did not improve from 0.10349\n","\n","Epoch 00356: val_loss did not improve from 0.10349\n","\n","Epoch 00357: val_loss did not improve from 0.10349\n","\n","Epoch 00358: val_loss improved from 0.10349 to 0.10240, saving model to ./model/358.0000-0.1024.hdf5\n","\n","Epoch 00359: val_loss did not improve from 0.10240\n","\n","Epoch 00360: val_loss did not improve from 0.10240\n","\n","Epoch 00361: val_loss did not improve from 0.10240\n","\n","Epoch 00362: val_loss did not improve from 0.10240\n","\n","Epoch 00363: val_loss did not improve from 0.10240\n","\n","Epoch 00364: val_loss did not improve from 0.10240\n","\n","Epoch 00365: val_loss improved from 0.10240 to 0.10161, saving model to ./model/365.0000-0.1016.hdf5\n","\n","Epoch 00366: val_loss did not improve from 0.10161\n","\n","Epoch 00367: val_loss did not improve from 0.10161\n","\n","Epoch 00368: val_loss did not improve from 0.10161\n","\n","Epoch 00369: val_loss improved from 0.10161 to 0.10040, saving model to ./model/369.0000-0.1004.hdf5\n","\n","Epoch 00370: val_loss did not improve from 0.10040\n","\n","Epoch 00371: val_loss did not improve from 0.10040\n","\n","Epoch 00372: val_loss did not improve from 0.10040\n","\n","Epoch 00373: val_loss did not improve from 0.10040\n","\n","Epoch 00374: val_loss did not improve from 0.10040\n","\n","Epoch 00375: val_loss did not improve from 0.10040\n","\n","Epoch 00376: val_loss did not improve from 0.10040\n","\n","Epoch 00377: val_loss did not improve from 0.10040\n","\n","Epoch 00378: val_loss did not improve from 0.10040\n","\n","Epoch 00379: val_loss did not improve from 0.10040\n","\n","Epoch 00380: val_loss improved from 0.10040 to 0.09901, saving model to ./model/380.0000-0.0990.hdf5\n","\n","Epoch 00381: val_loss did not improve from 0.09901\n","\n","Epoch 00382: val_loss did not improve from 0.09901\n","\n","Epoch 00383: val_loss did not improve from 0.09901\n","\n","Epoch 00384: val_loss did not improve from 0.09901\n","\n","Epoch 00385: val_loss did not improve from 0.09901\n","\n","Epoch 00386: val_loss improved from 0.09901 to 0.09865, saving model to ./model/386.0000-0.0987.hdf5\n","\n","Epoch 00387: val_loss did not improve from 0.09865\n","\n","Epoch 00388: val_loss did not improve from 0.09865\n","\n","Epoch 00389: val_loss did not improve from 0.09865\n","\n","Epoch 00390: val_loss improved from 0.09865 to 0.09782, saving model to ./model/390.0000-0.0978.hdf5\n","\n","Epoch 00391: val_loss improved from 0.09782 to 0.09704, saving model to ./model/391.0000-0.0970.hdf5\n","\n","Epoch 00392: val_loss did not improve from 0.09704\n","\n","Epoch 00393: val_loss did not improve from 0.09704\n","\n","Epoch 00394: val_loss did not improve from 0.09704\n","\n","Epoch 00395: val_loss did not improve from 0.09704\n","\n","Epoch 00396: val_loss did not improve from 0.09704\n","\n","Epoch 00397: val_loss did not improve from 0.09704\n","\n","Epoch 00398: val_loss did not improve from 0.09704\n","\n","Epoch 00399: val_loss did not improve from 0.09704\n","\n","Epoch 00400: val_loss did not improve from 0.09704\n","\n","Epoch 00401: val_loss did not improve from 0.09704\n","\n","Epoch 00402: val_loss did not improve from 0.09704\n","\n","Epoch 00403: val_loss did not improve from 0.09704\n","\n","Epoch 00404: val_loss did not improve from 0.09704\n","\n","Epoch 00405: val_loss did not improve from 0.09704\n","\n","Epoch 00406: val_loss did not improve from 0.09704\n","\n","Epoch 00407: val_loss did not improve from 0.09704\n","\n","Epoch 00408: val_loss did not improve from 0.09704\n","\n","Epoch 00409: val_loss did not improve from 0.09704\n","\n","Epoch 00410: val_loss did not improve from 0.09704\n","\n","Epoch 00411: val_loss did not improve from 0.09704\n","\n","Epoch 00412: val_loss improved from 0.09704 to 0.09672, saving model to ./model/412.0000-0.0967.hdf5\n","\n","Epoch 00413: val_loss did not improve from 0.09672\n","\n","Epoch 00414: val_loss did not improve from 0.09672\n","\n","Epoch 00415: val_loss did not improve from 0.09672\n","\n","Epoch 00416: val_loss did not improve from 0.09672\n","\n","Epoch 00417: val_loss improved from 0.09672 to 0.09585, saving model to ./model/417.0000-0.0958.hdf5\n","\n","Epoch 00418: val_loss did not improve from 0.09585\n","\n","Epoch 00419: val_loss did not improve from 0.09585\n","\n","Epoch 00420: val_loss did not improve from 0.09585\n","\n","Epoch 00421: val_loss did not improve from 0.09585\n","\n","Epoch 00422: val_loss improved from 0.09585 to 0.09560, saving model to ./model/422.0000-0.0956.hdf5\n","\n","Epoch 00423: val_loss improved from 0.09560 to 0.09503, saving model to ./model/423.0000-0.0950.hdf5\n","\n","Epoch 00424: val_loss did not improve from 0.09503\n","\n","Epoch 00425: val_loss did not improve from 0.09503\n","\n","Epoch 00426: val_loss did not improve from 0.09503\n","\n","Epoch 00427: val_loss did not improve from 0.09503\n","\n","Epoch 00428: val_loss improved from 0.09503 to 0.09371, saving model to ./model/428.0000-0.0937.hdf5\n","\n","Epoch 00429: val_loss did not improve from 0.09371\n","\n","Epoch 00430: val_loss did not improve from 0.09371\n","\n","Epoch 00431: val_loss did not improve from 0.09371\n","\n","Epoch 00432: val_loss improved from 0.09371 to 0.09268, saving model to ./model/432.0000-0.0927.hdf5\n","\n","Epoch 00433: val_loss did not improve from 0.09268\n","\n","Epoch 00434: val_loss did not improve from 0.09268\n","\n","Epoch 00435: val_loss did not improve from 0.09268\n","\n","Epoch 00436: val_loss improved from 0.09268 to 0.09204, saving model to ./model/436.0000-0.0920.hdf5\n","\n","Epoch 00437: val_loss did not improve from 0.09204\n","\n","Epoch 00438: val_loss did not improve from 0.09204\n","\n","Epoch 00439: val_loss did not improve from 0.09204\n","\n","Epoch 00440: val_loss improved from 0.09204 to 0.09131, saving model to ./model/440.0000-0.0913.hdf5\n","\n","Epoch 00441: val_loss did not improve from 0.09131\n","\n","Epoch 00442: val_loss did not improve from 0.09131\n","\n","Epoch 00443: val_loss did not improve from 0.09131\n","\n","Epoch 00444: val_loss improved from 0.09131 to 0.09097, saving model to ./model/444.0000-0.0910.hdf5\n","\n","Epoch 00445: val_loss did not improve from 0.09097\n","\n","Epoch 00446: val_loss did not improve from 0.09097\n","\n","Epoch 00447: val_loss did not improve from 0.09097\n","\n","Epoch 00448: val_loss improved from 0.09097 to 0.09001, saving model to ./model/448.0000-0.0900.hdf5\n","\n","Epoch 00449: val_loss did not improve from 0.09001\n","\n","Epoch 00450: val_loss did not improve from 0.09001\n","\n","Epoch 00451: val_loss did not improve from 0.09001\n","\n","Epoch 00452: val_loss did not improve from 0.09001\n","\n","Epoch 00453: val_loss did not improve from 0.09001\n","\n","Epoch 00454: val_loss did not improve from 0.09001\n","\n","Epoch 00455: val_loss did not improve from 0.09001\n","\n","Epoch 00456: val_loss improved from 0.09001 to 0.08933, saving model to ./model/456.0000-0.0893.hdf5\n","\n","Epoch 00457: val_loss improved from 0.08933 to 0.08899, saving model to ./model/457.0000-0.0890.hdf5\n","\n","Epoch 00458: val_loss did not improve from 0.08899\n","\n","Epoch 00459: val_loss did not improve from 0.08899\n","\n","Epoch 00460: val_loss did not improve from 0.08899\n","\n","Epoch 00461: val_loss improved from 0.08899 to 0.08865, saving model to ./model/461.0000-0.0886.hdf5\n","\n","Epoch 00462: val_loss did not improve from 0.08865\n","\n","Epoch 00463: val_loss did not improve from 0.08865\n","\n","Epoch 00464: val_loss did not improve from 0.08865\n","\n","Epoch 00465: val_loss did not improve from 0.08865\n","\n","Epoch 00466: val_loss did not improve from 0.08865\n","\n","Epoch 00467: val_loss did not improve from 0.08865\n","\n","Epoch 00468: val_loss did not improve from 0.08865\n","\n","Epoch 00469: val_loss did not improve from 0.08865\n","\n","Epoch 00470: val_loss did not improve from 0.08865\n","\n","Epoch 00471: val_loss did not improve from 0.08865\n","\n","Epoch 00472: val_loss did not improve from 0.08865\n","\n","Epoch 00473: val_loss did not improve from 0.08865\n","\n","Epoch 00474: val_loss improved from 0.08865 to 0.08839, saving model to ./model/474.0000-0.0884.hdf5\n","\n","Epoch 00475: val_loss improved from 0.08839 to 0.08762, saving model to ./model/475.0000-0.0876.hdf5\n","\n","Epoch 00476: val_loss did not improve from 0.08762\n","\n","Epoch 00477: val_loss did not improve from 0.08762\n","\n","Epoch 00478: val_loss did not improve from 0.08762\n","\n","Epoch 00479: val_loss improved from 0.08762 to 0.08686, saving model to ./model/479.0000-0.0869.hdf5\n","\n","Epoch 00480: val_loss did not improve from 0.08686\n","\n","Epoch 00481: val_loss did not improve from 0.08686\n","\n","Epoch 00482: val_loss did not improve from 0.08686\n","\n","Epoch 00483: val_loss improved from 0.08686 to 0.08656, saving model to ./model/483.0000-0.0866.hdf5\n","\n","Epoch 00484: val_loss did not improve from 0.08656\n","\n","Epoch 00485: val_loss did not improve from 0.08656\n","\n","Epoch 00486: val_loss did not improve from 0.08656\n","\n","Epoch 00487: val_loss improved from 0.08656 to 0.08643, saving model to ./model/487.0000-0.0864.hdf5\n","\n","Epoch 00488: val_loss did not improve from 0.08643\n","\n","Epoch 00489: val_loss did not improve from 0.08643\n","\n","Epoch 00490: val_loss did not improve from 0.08643\n","\n","Epoch 00491: val_loss improved from 0.08643 to 0.08612, saving model to ./model/491.0000-0.0861.hdf5\n","\n","Epoch 00492: val_loss did not improve from 0.08612\n","\n","Epoch 00493: val_loss did not improve from 0.08612\n","\n","Epoch 00494: val_loss did not improve from 0.08612\n","\n","Epoch 00495: val_loss did not improve from 0.08612\n","\n","Epoch 00496: val_loss did not improve from 0.08612\n","\n","Epoch 00497: val_loss did not improve from 0.08612\n","\n","Epoch 00498: val_loss did not improve from 0.08612\n","\n","Epoch 00499: val_loss did not improve from 0.08612\n","\n","Epoch 00500: val_loss did not improve from 0.08612\n","\n","Epoch 00501: val_loss did not improve from 0.08612\n","\n","Epoch 00502: val_loss improved from 0.08612 to 0.08530, saving model to ./model/502.0000-0.0853.hdf5\n","\n","Epoch 00503: val_loss did not improve from 0.08530\n","\n","Epoch 00504: val_loss did not improve from 0.08530\n","\n","Epoch 00505: val_loss did not improve from 0.08530\n","\n","Epoch 00506: val_loss did not improve from 0.08530\n","\n","Epoch 00507: val_loss did not improve from 0.08530\n","\n","Epoch 00508: val_loss did not improve from 0.08530\n","\n","Epoch 00509: val_loss did not improve from 0.08530\n","\n","Epoch 00510: val_loss did not improve from 0.08530\n","\n","Epoch 00511: val_loss improved from 0.08530 to 0.08525, saving model to ./model/511.0000-0.0852.hdf5\n","\n","Epoch 00512: val_loss improved from 0.08525 to 0.08486, saving model to ./model/512.0000-0.0849.hdf5\n","\n","Epoch 00513: val_loss did not improve from 0.08486\n","\n","Epoch 00514: val_loss did not improve from 0.08486\n","\n","Epoch 00515: val_loss did not improve from 0.08486\n","\n","Epoch 00516: val_loss improved from 0.08486 to 0.08439, saving model to ./model/516.0000-0.0844.hdf5\n","\n","Epoch 00517: val_loss did not improve from 0.08439\n","\n","Epoch 00518: val_loss did not improve from 0.08439\n","\n","Epoch 00519: val_loss did not improve from 0.08439\n","\n","Epoch 00520: val_loss did not improve from 0.08439\n","\n","Epoch 00521: val_loss did not improve from 0.08439\n","\n","Epoch 00522: val_loss did not improve from 0.08439\n","\n","Epoch 00523: val_loss did not improve from 0.08439\n","\n","Epoch 00524: val_loss improved from 0.08439 to 0.08411, saving model to ./model/524.0000-0.0841.hdf5\n","\n","Epoch 00525: val_loss did not improve from 0.08411\n","\n","Epoch 00526: val_loss did not improve from 0.08411\n","\n","Epoch 00527: val_loss did not improve from 0.08411\n","\n","Epoch 00528: val_loss improved from 0.08411 to 0.08304, saving model to ./model/528.0000-0.0830.hdf5\n","\n","Epoch 00529: val_loss did not improve from 0.08304\n","\n","Epoch 00530: val_loss did not improve from 0.08304\n","\n","Epoch 00531: val_loss did not improve from 0.08304\n","\n","Epoch 00532: val_loss improved from 0.08304 to 0.08271, saving model to ./model/532.0000-0.0827.hdf5\n","\n","Epoch 00533: val_loss did not improve from 0.08271\n","\n","Epoch 00534: val_loss did not improve from 0.08271\n","\n","Epoch 00535: val_loss did not improve from 0.08271\n","\n","Epoch 00536: val_loss did not improve from 0.08271\n","\n","Epoch 00537: val_loss did not improve from 0.08271\n","\n","Epoch 00538: val_loss did not improve from 0.08271\n","\n","Epoch 00539: val_loss did not improve from 0.08271\n","\n","Epoch 00540: val_loss did not improve from 0.08271\n","\n","Epoch 00541: val_loss improved from 0.08271 to 0.08224, saving model to ./model/541.0000-0.0822.hdf5\n","\n","Epoch 00542: val_loss did not improve from 0.08224\n","\n","Epoch 00543: val_loss did not improve from 0.08224\n","\n","Epoch 00544: val_loss did not improve from 0.08224\n","\n","Epoch 00545: val_loss improved from 0.08224 to 0.08209, saving model to ./model/545.0000-0.0821.hdf5\n","\n","Epoch 00546: val_loss did not improve from 0.08209\n","\n","Epoch 00547: val_loss did not improve from 0.08209\n","\n","Epoch 00548: val_loss did not improve from 0.08209\n","\n","Epoch 00549: val_loss did not improve from 0.08209\n","\n","Epoch 00550: val_loss did not improve from 0.08209\n","\n","Epoch 00551: val_loss did not improve from 0.08209\n","\n","Epoch 00552: val_loss did not improve from 0.08209\n","\n","Epoch 00553: val_loss did not improve from 0.08209\n","\n","Epoch 00554: val_loss did not improve from 0.08209\n","\n","Epoch 00555: val_loss did not improve from 0.08209\n","\n","Epoch 00556: val_loss did not improve from 0.08209\n","\n","Epoch 00557: val_loss did not improve from 0.08209\n","\n","Epoch 00558: val_loss did not improve from 0.08209\n","\n","Epoch 00559: val_loss did not improve from 0.08209\n","\n","Epoch 00560: val_loss did not improve from 0.08209\n","\n","Epoch 00561: val_loss did not improve from 0.08209\n","\n","Epoch 00562: val_loss did not improve from 0.08209\n","\n","Epoch 00563: val_loss did not improve from 0.08209\n","\n","Epoch 00564: val_loss did not improve from 0.08209\n","\n","Epoch 00565: val_loss did not improve from 0.08209\n","\n","Epoch 00566: val_loss did not improve from 0.08209\n","\n","Epoch 00567: val_loss improved from 0.08209 to 0.08190, saving model to ./model/567.0000-0.0819.hdf5\n","\n","Epoch 00568: val_loss did not improve from 0.08190\n","\n","Epoch 00569: val_loss did not improve from 0.08190\n","\n","Epoch 00570: val_loss did not improve from 0.08190\n","\n","Epoch 00571: val_loss did not improve from 0.08190\n","\n","Epoch 00572: val_loss did not improve from 0.08190\n","\n","Epoch 00573: val_loss did not improve from 0.08190\n","\n","Epoch 00574: val_loss did not improve from 0.08190\n","\n","Epoch 00575: val_loss did not improve from 0.08190\n","\n","Epoch 00576: val_loss did not improve from 0.08190\n","\n","Epoch 00577: val_loss did not improve from 0.08190\n","\n","Epoch 00578: val_loss did not improve from 0.08190\n","\n","Epoch 00579: val_loss did not improve from 0.08190\n","\n","Epoch 00580: val_loss improved from 0.08190 to 0.08157, saving model to ./model/580.0000-0.0816.hdf5\n","\n","Epoch 00581: val_loss did not improve from 0.08157\n","\n","Epoch 00582: val_loss did not improve from 0.08157\n","\n","Epoch 00583: val_loss did not improve from 0.08157\n","\n","Epoch 00584: val_loss did not improve from 0.08157\n","\n","Epoch 00585: val_loss did not improve from 0.08157\n","\n","Epoch 00586: val_loss did not improve from 0.08157\n","\n","Epoch 00587: val_loss improved from 0.08157 to 0.08038, saving model to ./model/587.0000-0.0804.hdf5\n","\n","Epoch 00588: val_loss did not improve from 0.08038\n","\n","Epoch 00589: val_loss did not improve from 0.08038\n","\n","Epoch 00590: val_loss did not improve from 0.08038\n","\n","Epoch 00591: val_loss did not improve from 0.08038\n","\n","Epoch 00592: val_loss did not improve from 0.08038\n","\n","Epoch 00593: val_loss did not improve from 0.08038\n","\n","Epoch 00594: val_loss did not improve from 0.08038\n","\n","Epoch 00595: val_loss did not improve from 0.08038\n","\n","Epoch 00596: val_loss did not improve from 0.08038\n","\n","Epoch 00597: val_loss did not improve from 0.08038\n","\n","Epoch 00598: val_loss did not improve from 0.08038\n","\n","Epoch 00599: val_loss did not improve from 0.08038\n","\n","Epoch 00600: val_loss improved from 0.08038 to 0.07908, saving model to ./model/600.0000-0.0791.hdf5\n","\n","Epoch 00601: val_loss did not improve from 0.07908\n","\n","Epoch 00602: val_loss did not improve from 0.07908\n","\n","Epoch 00603: val_loss did not improve from 0.07908\n","\n","Epoch 00604: val_loss did not improve from 0.07908\n","\n","Epoch 00605: val_loss did not improve from 0.07908\n","\n","Epoch 00606: val_loss did not improve from 0.07908\n","\n","Epoch 00607: val_loss did not improve from 0.07908\n","\n","Epoch 00608: val_loss did not improve from 0.07908\n","\n","Epoch 00609: val_loss did not improve from 0.07908\n","\n","Epoch 00610: val_loss did not improve from 0.07908\n","\n","Epoch 00611: val_loss did not improve from 0.07908\n","\n","Epoch 00612: val_loss improved from 0.07908 to 0.07872, saving model to ./model/612.0000-0.0787.hdf5\n","\n","Epoch 00613: val_loss did not improve from 0.07872\n","\n","Epoch 00614: val_loss did not improve from 0.07872\n","\n","Epoch 00615: val_loss did not improve from 0.07872\n","\n","Epoch 00616: val_loss improved from 0.07872 to 0.07810, saving model to ./model/616.0000-0.0781.hdf5\n","\n","Epoch 00617: val_loss did not improve from 0.07810\n","\n","Epoch 00618: val_loss did not improve from 0.07810\n","\n","Epoch 00619: val_loss did not improve from 0.07810\n","\n","Epoch 00620: val_loss improved from 0.07810 to 0.07809, saving model to ./model/620.0000-0.0781.hdf5\n","\n","Epoch 00621: val_loss did not improve from 0.07809\n","\n","Epoch 00622: val_loss did not improve from 0.07809\n","\n","Epoch 00623: val_loss did not improve from 0.07809\n","\n","Epoch 00624: val_loss did not improve from 0.07809\n","\n","Epoch 00625: val_loss did not improve from 0.07809\n","\n","Epoch 00626: val_loss did not improve from 0.07809\n","\n","Epoch 00627: val_loss did not improve from 0.07809\n","\n","Epoch 00628: val_loss did not improve from 0.07809\n","\n","Epoch 00629: val_loss did not improve from 0.07809\n","\n","Epoch 00630: val_loss did not improve from 0.07809\n","\n","Epoch 00631: val_loss did not improve from 0.07809\n","\n","Epoch 00632: val_loss did not improve from 0.07809\n","\n","Epoch 00633: val_loss did not improve from 0.07809\n","\n","Epoch 00634: val_loss did not improve from 0.07809\n","\n","Epoch 00635: val_loss did not improve from 0.07809\n","\n","Epoch 00636: val_loss did not improve from 0.07809\n","\n","Epoch 00637: val_loss did not improve from 0.07809\n","\n","Epoch 00638: val_loss did not improve from 0.07809\n","\n","Epoch 00639: val_loss did not improve from 0.07809\n","\n","Epoch 00640: val_loss did not improve from 0.07809\n","\n","Epoch 00641: val_loss did not improve from 0.07809\n","\n","Epoch 00642: val_loss did not improve from 0.07809\n","\n","Epoch 00643: val_loss did not improve from 0.07809\n","\n","Epoch 00644: val_loss improved from 0.07809 to 0.07775, saving model to ./model/644.0000-0.0778.hdf5\n","\n","Epoch 00645: val_loss did not improve from 0.07775\n","\n","Epoch 00646: val_loss did not improve from 0.07775\n","\n","Epoch 00647: val_loss did not improve from 0.07775\n","\n","Epoch 00648: val_loss improved from 0.07775 to 0.07685, saving model to ./model/648.0000-0.0768.hdf5\n","\n","Epoch 00649: val_loss did not improve from 0.07685\n","\n","Epoch 00650: val_loss did not improve from 0.07685\n","\n","Epoch 00651: val_loss did not improve from 0.07685\n","\n","Epoch 00652: val_loss improved from 0.07685 to 0.07681, saving model to ./model/652.0000-0.0768.hdf5\n","\n","Epoch 00653: val_loss did not improve from 0.07681\n","\n","Epoch 00654: val_loss did not improve from 0.07681\n","\n","Epoch 00655: val_loss did not improve from 0.07681\n","\n","Epoch 00656: val_loss did not improve from 0.07681\n","\n","Epoch 00657: val_loss did not improve from 0.07681\n","\n","Epoch 00658: val_loss did not improve from 0.07681\n","\n","Epoch 00659: val_loss did not improve from 0.07681\n","\n","Epoch 00660: val_loss did not improve from 0.07681\n","\n","Epoch 00661: val_loss did not improve from 0.07681\n","\n","Epoch 00662: val_loss did not improve from 0.07681\n","\n","Epoch 00663: val_loss did not improve from 0.07681\n","\n","Epoch 00664: val_loss did not improve from 0.07681\n","\n","Epoch 00665: val_loss did not improve from 0.07681\n","\n","Epoch 00666: val_loss did not improve from 0.07681\n","\n","Epoch 00667: val_loss did not improve from 0.07681\n","\n","Epoch 00668: val_loss did not improve from 0.07681\n","\n","Epoch 00669: val_loss did not improve from 0.07681\n","\n","Epoch 00670: val_loss did not improve from 0.07681\n","\n","Epoch 00671: val_loss did not improve from 0.07681\n","\n","Epoch 00672: val_loss did not improve from 0.07681\n","\n","Epoch 00673: val_loss did not improve from 0.07681\n","\n","Epoch 00674: val_loss did not improve from 0.07681\n","\n","Epoch 00675: val_loss did not improve from 0.07681\n","\n","Epoch 00676: val_loss did not improve from 0.07681\n","\n","Epoch 00677: val_loss did not improve from 0.07681\n","\n","Epoch 00678: val_loss did not improve from 0.07681\n","\n","Epoch 00679: val_loss did not improve from 0.07681\n","\n","Epoch 00680: val_loss did not improve from 0.07681\n","\n","Epoch 00681: val_loss did not improve from 0.07681\n","\n","Epoch 00682: val_loss did not improve from 0.07681\n","\n","Epoch 00683: val_loss did not improve from 0.07681\n","\n","Epoch 00684: val_loss did not improve from 0.07681\n","\n","Epoch 00685: val_loss did not improve from 0.07681\n","\n","Epoch 00686: val_loss did not improve from 0.07681\n","\n","Epoch 00687: val_loss did not improve from 0.07681\n","\n","Epoch 00688: val_loss did not improve from 0.07681\n","\n","Epoch 00689: val_loss did not improve from 0.07681\n","\n","Epoch 00690: val_loss did not improve from 0.07681\n","\n","Epoch 00691: val_loss did not improve from 0.07681\n","\n","Epoch 00692: val_loss did not improve from 0.07681\n","\n","Epoch 00693: val_loss did not improve from 0.07681\n","\n","Epoch 00694: val_loss did not improve from 0.07681\n","\n","Epoch 00695: val_loss improved from 0.07681 to 0.07627, saving model to ./model/695.0000-0.0763.hdf5\n","\n","Epoch 00696: val_loss did not improve from 0.07627\n","\n","Epoch 00697: val_loss did not improve from 0.07627\n","\n","Epoch 00698: val_loss did not improve from 0.07627\n","\n","Epoch 00699: val_loss improved from 0.07627 to 0.07609, saving model to ./model/699.0000-0.0761.hdf5\n","\n","Epoch 00700: val_loss improved from 0.07609 to 0.07493, saving model to ./model/700.0000-0.0749.hdf5\n","\n","Epoch 00701: val_loss did not improve from 0.07493\n","\n","Epoch 00702: val_loss did not improve from 0.07493\n","\n","Epoch 00703: val_loss did not improve from 0.07493\n","\n","Epoch 00704: val_loss did not improve from 0.07493\n","\n","Epoch 00705: val_loss did not improve from 0.07493\n","\n","Epoch 00706: val_loss did not improve from 0.07493\n","\n","Epoch 00707: val_loss improved from 0.07493 to 0.07385, saving model to ./model/707.0000-0.0739.hdf5\n","\n","Epoch 00708: val_loss did not improve from 0.07385\n","\n","Epoch 00709: val_loss did not improve from 0.07385\n","\n","Epoch 00710: val_loss did not improve from 0.07385\n","\n","Epoch 00711: val_loss improved from 0.07385 to 0.07346, saving model to ./model/711.0000-0.0735.hdf5\n","\n","Epoch 00712: val_loss did not improve from 0.07346\n","\n","Epoch 00713: val_loss did not improve from 0.07346\n","\n","Epoch 00714: val_loss did not improve from 0.07346\n","\n","Epoch 00715: val_loss did not improve from 0.07346\n","\n","Epoch 00716: val_loss did not improve from 0.07346\n","\n","Epoch 00717: val_loss did not improve from 0.07346\n","\n","Epoch 00718: val_loss did not improve from 0.07346\n","\n","Epoch 00719: val_loss did not improve from 0.07346\n","\n","Epoch 00720: val_loss did not improve from 0.07346\n","\n","Epoch 00721: val_loss did not improve from 0.07346\n","\n","Epoch 00722: val_loss did not improve from 0.07346\n","\n","Epoch 00723: val_loss did not improve from 0.07346\n","\n","Epoch 00724: val_loss did not improve from 0.07346\n","\n","Epoch 00725: val_loss did not improve from 0.07346\n","\n","Epoch 00726: val_loss did not improve from 0.07346\n","\n","Epoch 00727: val_loss did not improve from 0.07346\n","\n","Epoch 00728: val_loss did not improve from 0.07346\n","\n","Epoch 00729: val_loss did not improve from 0.07346\n","\n","Epoch 00730: val_loss did not improve from 0.07346\n","\n","Epoch 00731: val_loss did not improve from 0.07346\n","\n","Epoch 00732: val_loss did not improve from 0.07346\n","\n","Epoch 00733: val_loss did not improve from 0.07346\n","\n","Epoch 00734: val_loss did not improve from 0.07346\n","\n","Epoch 00735: val_loss did not improve from 0.07346\n","\n","Epoch 00736: val_loss did not improve from 0.07346\n","\n","Epoch 00737: val_loss did not improve from 0.07346\n","\n","Epoch 00738: val_loss did not improve from 0.07346\n","\n","Epoch 00739: val_loss improved from 0.07346 to 0.07284, saving model to ./model/739.0000-0.0728.hdf5\n","\n","Epoch 00740: val_loss did not improve from 0.07284\n","\n","Epoch 00741: val_loss did not improve from 0.07284\n","\n","Epoch 00742: val_loss did not improve from 0.07284\n","\n","Epoch 00743: val_loss did not improve from 0.07284\n","\n","Epoch 00744: val_loss did not improve from 0.07284\n","\n","Epoch 00745: val_loss did not improve from 0.07284\n","\n","Epoch 00746: val_loss did not improve from 0.07284\n","\n","Epoch 00747: val_loss did not improve from 0.07284\n","\n","Epoch 00748: val_loss did not improve from 0.07284\n","\n","Epoch 00749: val_loss did not improve from 0.07284\n","\n","Epoch 00750: val_loss did not improve from 0.07284\n","\n","Epoch 00751: val_loss did not improve from 0.07284\n","\n","Epoch 00752: val_loss did not improve from 0.07284\n","\n","Epoch 00753: val_loss did not improve from 0.07284\n","\n","Epoch 00754: val_loss did not improve from 0.07284\n","\n","Epoch 00755: val_loss did not improve from 0.07284\n","\n","Epoch 00756: val_loss did not improve from 0.07284\n","\n","Epoch 00757: val_loss did not improve from 0.07284\n","\n","Epoch 00758: val_loss did not improve from 0.07284\n","\n","Epoch 00759: val_loss did not improve from 0.07284\n","\n","Epoch 00760: val_loss did not improve from 0.07284\n","\n","Epoch 00761: val_loss did not improve from 0.07284\n","\n","Epoch 00762: val_loss did not improve from 0.07284\n","\n","Epoch 00763: val_loss did not improve from 0.07284\n","\n","Epoch 00764: val_loss did not improve from 0.07284\n","\n","Epoch 00765: val_loss did not improve from 0.07284\n","\n","Epoch 00766: val_loss did not improve from 0.07284\n","\n","Epoch 00767: val_loss did not improve from 0.07284\n","\n","Epoch 00768: val_loss did not improve from 0.07284\n","\n","Epoch 00769: val_loss did not improve from 0.07284\n","\n","Epoch 00770: val_loss did not improve from 0.07284\n","\n","Epoch 00771: val_loss did not improve from 0.07284\n","\n","Epoch 00772: val_loss did not improve from 0.07284\n","\n","Epoch 00773: val_loss did not improve from 0.07284\n","\n","Epoch 00774: val_loss did not improve from 0.07284\n","\n","Epoch 00775: val_loss did not improve from 0.07284\n","\n","Epoch 00776: val_loss did not improve from 0.07284\n","\n","Epoch 00777: val_loss did not improve from 0.07284\n","\n","Epoch 00778: val_loss did not improve from 0.07284\n","\n","Epoch 00779: val_loss did not improve from 0.07284\n","\n","Epoch 00780: val_loss did not improve from 0.07284\n","\n","Epoch 00781: val_loss did not improve from 0.07284\n","\n","Epoch 00782: val_loss did not improve from 0.07284\n","\n","Epoch 00783: val_loss did not improve from 0.07284\n","\n","Epoch 00784: val_loss did not improve from 0.07284\n","\n","Epoch 00785: val_loss did not improve from 0.07284\n","\n","Epoch 00786: val_loss did not improve from 0.07284\n","\n","Epoch 00787: val_loss did not improve from 0.07284\n","\n","Epoch 00788: val_loss did not improve from 0.07284\n","\n","Epoch 00789: val_loss did not improve from 0.07284\n","\n","Epoch 00790: val_loss did not improve from 0.07284\n","\n","Epoch 00791: val_loss did not improve from 0.07284\n","\n","Epoch 00792: val_loss did not improve from 0.07284\n","\n","Epoch 00793: val_loss did not improve from 0.07284\n","\n","Epoch 00794: val_loss did not improve from 0.07284\n","\n","Epoch 00795: val_loss did not improve from 0.07284\n","\n","Epoch 00796: val_loss did not improve from 0.07284\n","\n","Epoch 00797: val_loss did not improve from 0.07284\n","\n","Epoch 00798: val_loss did not improve from 0.07284\n","\n","Epoch 00799: val_loss did not improve from 0.07284\n","\n","Epoch 00800: val_loss did not improve from 0.07284\n","\n","Epoch 00801: val_loss did not improve from 0.07284\n","\n","Epoch 00802: val_loss did not improve from 0.07284\n","\n","Epoch 00803: val_loss did not improve from 0.07284\n","\n","Epoch 00804: val_loss did not improve from 0.07284\n","\n","Epoch 00805: val_loss did not improve from 0.07284\n","\n","Epoch 00806: val_loss did not improve from 0.07284\n","\n","Epoch 00807: val_loss did not improve from 0.07284\n","\n","Epoch 00808: val_loss did not improve from 0.07284\n","\n","Epoch 00809: val_loss did not improve from 0.07284\n","\n","Epoch 00810: val_loss improved from 0.07284 to 0.07242, saving model to ./model/810.0000-0.0724.hdf5\n","\n","Epoch 00811: val_loss did not improve from 0.07242\n","\n","Epoch 00812: val_loss did not improve from 0.07242\n","\n","Epoch 00813: val_loss did not improve from 0.07242\n","\n","Epoch 00814: val_loss improved from 0.07242 to 0.07149, saving model to ./model/814.0000-0.0715.hdf5\n","\n","Epoch 00815: val_loss did not improve from 0.07149\n","\n","Epoch 00816: val_loss did not improve from 0.07149\n","\n","Epoch 00817: val_loss did not improve from 0.07149\n","\n","Epoch 00818: val_loss did not improve from 0.07149\n","\n","Epoch 00819: val_loss did not improve from 0.07149\n","\n","Epoch 00820: val_loss did not improve from 0.07149\n","\n","Epoch 00821: val_loss did not improve from 0.07149\n","\n","Epoch 00822: val_loss did not improve from 0.07149\n","\n","Epoch 00823: val_loss did not improve from 0.07149\n","\n","Epoch 00824: val_loss did not improve from 0.07149\n","\n","Epoch 00825: val_loss did not improve from 0.07149\n","\n","Epoch 00826: val_loss did not improve from 0.07149\n","\n","Epoch 00827: val_loss did not improve from 0.07149\n","\n","Epoch 00828: val_loss did not improve from 0.07149\n","\n","Epoch 00829: val_loss did not improve from 0.07149\n","\n","Epoch 00830: val_loss did not improve from 0.07149\n","\n","Epoch 00831: val_loss did not improve from 0.07149\n","\n","Epoch 00832: val_loss did not improve from 0.07149\n","\n","Epoch 00833: val_loss did not improve from 0.07149\n","\n","Epoch 00834: val_loss did not improve from 0.07149\n","\n","Epoch 00835: val_loss did not improve from 0.07149\n","\n","Epoch 00836: val_loss did not improve from 0.07149\n","\n","Epoch 00837: val_loss did not improve from 0.07149\n","\n","Epoch 00838: val_loss did not improve from 0.07149\n","\n","Epoch 00839: val_loss did not improve from 0.07149\n","\n","Epoch 00840: val_loss did not improve from 0.07149\n","\n","Epoch 00841: val_loss did not improve from 0.07149\n","\n","Epoch 00842: val_loss did not improve from 0.07149\n","\n","Epoch 00843: val_loss did not improve from 0.07149\n","\n","Epoch 00844: val_loss did not improve from 0.07149\n","\n","Epoch 00845: val_loss did not improve from 0.07149\n","\n","Epoch 00846: val_loss did not improve from 0.07149\n","\n","Epoch 00847: val_loss did not improve from 0.07149\n","\n","Epoch 00848: val_loss did not improve from 0.07149\n","\n","Epoch 00849: val_loss did not improve from 0.07149\n","\n","Epoch 00850: val_loss did not improve from 0.07149\n","\n","Epoch 00851: val_loss did not improve from 0.07149\n","\n","Epoch 00852: val_loss did not improve from 0.07149\n","\n","Epoch 00853: val_loss did not improve from 0.07149\n","\n","Epoch 00854: val_loss did not improve from 0.07149\n","\n","Epoch 00855: val_loss did not improve from 0.07149\n","\n","Epoch 00856: val_loss did not improve from 0.07149\n","\n","Epoch 00857: val_loss did not improve from 0.07149\n","\n","Epoch 00858: val_loss did not improve from 0.07149\n","\n","Epoch 00859: val_loss did not improve from 0.07149\n","\n","Epoch 00860: val_loss did not improve from 0.07149\n","\n","Epoch 00861: val_loss did not improve from 0.07149\n","\n","Epoch 00862: val_loss did not improve from 0.07149\n","\n","Epoch 00863: val_loss did not improve from 0.07149\n","\n","Epoch 00864: val_loss did not improve from 0.07149\n","\n","Epoch 00865: val_loss did not improve from 0.07149\n","\n","Epoch 00866: val_loss did not improve from 0.07149\n","\n","Epoch 00867: val_loss did not improve from 0.07149\n","\n","Epoch 00868: val_loss did not improve from 0.07149\n","\n","Epoch 00869: val_loss did not improve from 0.07149\n","\n","Epoch 00870: val_loss did not improve from 0.07149\n","\n","Epoch 00871: val_loss did not improve from 0.07149\n","\n","Epoch 00872: val_loss did not improve from 0.07149\n","\n","Epoch 00873: val_loss did not improve from 0.07149\n","\n","Epoch 00874: val_loss did not improve from 0.07149\n","\n","Epoch 00875: val_loss did not improve from 0.07149\n","\n","Epoch 00876: val_loss did not improve from 0.07149\n","\n","Epoch 00877: val_loss did not improve from 0.07149\n","\n","Epoch 00878: val_loss did not improve from 0.07149\n","\n","Epoch 00879: val_loss did not improve from 0.07149\n","\n","Epoch 00880: val_loss did not improve from 0.07149\n","\n","Epoch 00881: val_loss did not improve from 0.07149\n","\n","Epoch 00882: val_loss did not improve from 0.07149\n","\n","Epoch 00883: val_loss did not improve from 0.07149\n","\n","Epoch 00884: val_loss did not improve from 0.07149\n","\n","Epoch 00885: val_loss did not improve from 0.07149\n","\n","Epoch 00886: val_loss did not improve from 0.07149\n","\n","Epoch 00887: val_loss did not improve from 0.07149\n","\n","Epoch 00888: val_loss did not improve from 0.07149\n","\n","Epoch 00889: val_loss did not improve from 0.07149\n","\n","Epoch 00890: val_loss did not improve from 0.07149\n","\n","Epoch 00891: val_loss did not improve from 0.07149\n","\n","Epoch 00892: val_loss did not improve from 0.07149\n","\n","Epoch 00893: val_loss did not improve from 0.07149\n","\n","Epoch 00894: val_loss did not improve from 0.07149\n","\n","Epoch 00895: val_loss did not improve from 0.07149\n","\n","Epoch 00896: val_loss did not improve from 0.07149\n","\n","Epoch 00897: val_loss did not improve from 0.07149\n","\n","Epoch 00898: val_loss did not improve from 0.07149\n","\n","Epoch 00899: val_loss did not improve from 0.07149\n","\n","Epoch 00900: val_loss did not improve from 0.07149\n","\n","Epoch 00901: val_loss did not improve from 0.07149\n","\n","Epoch 00902: val_loss did not improve from 0.07149\n","\n","Epoch 00903: val_loss did not improve from 0.07149\n","\n","Epoch 00904: val_loss did not improve from 0.07149\n","\n","Epoch 00905: val_loss did not improve from 0.07149\n","\n","Epoch 00906: val_loss did not improve from 0.07149\n","\n","Epoch 00907: val_loss did not improve from 0.07149\n","\n","Epoch 00908: val_loss did not improve from 0.07149\n","\n","Epoch 00909: val_loss did not improve from 0.07149\n","\n","Epoch 00910: val_loss did not improve from 0.07149\n","\n","Epoch 00911: val_loss did not improve from 0.07149\n","\n","Epoch 00912: val_loss did not improve from 0.07149\n","\n","Epoch 00913: val_loss did not improve from 0.07149\n","\n","Epoch 00914: val_loss did not improve from 0.07149\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.callbacks.History at 0x7f0c8966a3c8>"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"9WnzlhN3ztpI","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1594353055858,"user_tz":-540,"elapsed":40608,"user":{"displayName":"Jeong Tae Kim","photoUrl":"","userId":"09951578752883725793"}}},"source":[""],"execution_count":null,"outputs":[]}]}